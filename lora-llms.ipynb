{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:39:59.985703Z",
     "iopub.status.busy": "2024-11-16T13:39:59.985429Z",
     "iopub.status.idle": "2024-11-16T13:40:30.352148Z",
     "shell.execute_reply": "2024-11-16T13:40:30.350999Z",
     "shell.execute_reply.started": "2024-11-16T13:39:59.985671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.13.2\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.44.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install peft\n",
    "# !pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-16T13:40:30.355943Z",
     "iopub.status.busy": "2024-11-16T13:40:30.354620Z",
     "iopub.status.idle": "2024-11-16T13:42:00.700703Z",
     "shell.execute_reply": "2024-11-16T13:42:00.699820Z",
     "shell.execute_reply.started": "2024-11-16T13:40:30.355901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d3bb1d1aba437d8d5b389f308fbb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed593a6e6bb74165a7d66b6163d2d2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/6.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadd1353d5ee4a598c8e8cffa056f92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7479d8d39914cd3880db5ff2a3105af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2508e9df86142738fd6290a6ad4437d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "# import torch \n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "# base_model = 'model-attribution-challenge/bloom-2b5'\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "#                                              quantization_config=bnb_config,\n",
    "#                                              torch_dtype=torch.float16,\n",
    "#                                              device_map='auto'\n",
    "#                                             )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:42:00.702293Z",
     "iopub.status.busy": "2024-11-16T13:42:00.701858Z",
     "iopub.status.idle": "2024-11-16T13:42:00.706564Z",
     "shell.execute_reply": "2024-11-16T13:42:00.705666Z",
     "shell.execute_reply.started": "2024-11-16T13:42:00.702259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:42:00.708162Z",
     "iopub.status.busy": "2024-11-16T13:42:00.707877Z",
     "iopub.status.idle": "2024-11-16T13:42:00.723405Z",
     "shell.execute_reply": "2024-11-16T13:42:00.722566Z",
     "shell.execute_reply.started": "2024-11-16T13:42:00.708131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 2560)\n",
       "    (word_embeddings_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-29): 30 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear4bit(in_features=2560, out_features=7680, bias=True)\n",
       "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:42:00.726147Z",
     "iopub.status.busy": "2024-11-16T13:42:00.725869Z",
     "iopub.status.idle": "2024-11-16T13:42:01.083945Z",
     "shell.execute_reply": "2024-11-16T13:42:01.083149Z",
     "shell.execute_reply.started": "2024-11-16T13:42:00.726117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #QLoRA\n",
    "# from peft import get_peft_model, LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     bias='none',\n",
    "#     lora_dropout=0.05,\n",
    "#     task_type='CAUSAL_LM',\n",
    "#     target_modules=['dense', 'query_key_value'],\n",
    "# )\n",
    "\n",
    "# lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:42:01.085879Z",
     "iopub.status.busy": "2024-11-16T13:42:01.085272Z",
     "iopub.status.idle": "2024-11-16T13:42:01.094596Z",
     "shell.execute_reply": "2024-11-16T13:42:01.093694Z",
     "shell.execute_reply.started": "2024-11-16T13:42:01.085833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,372,800 || all params: 3,009,930,240 || trainable%: 0.2449\n"
     ]
    }
   ],
   "source": [
    "# #sô lượng tham số cần train\n",
    "# lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:42:01.096165Z",
     "iopub.status.busy": "2024-11-16T13:42:01.095813Z",
     "iopub.status.idle": "2024-11-16T13:42:12.585306Z",
     "shell.execute_reply": "2024-11-16T13:42:12.584137Z",
     "shell.execute_reply.started": "2024-11-16T13:42:01.096118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:42:12.587170Z",
     "iopub.status.busy": "2024-11-16T13:42:12.586813Z",
     "iopub.status.idle": "2024-11-16T13:42:14.227868Z",
     "shell.execute_reply": "2024-11-16T13:42:14.226951Z",
     "shell.execute_reply.started": "2024-11-16T13:42:12.587133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501407f876c14c3b8184d31ca9094a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31042d0ff02944f0b9ad708d31770217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661ba40b0b28450c87688c3b409c8952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "# paths = ['/kaggle/input/data-qa-python-06102004/Data-QA-Python.csv', '/kaggle/input/data-qa-py/data-gpt-create-6-10-2024.csv', '/kaggle/input/datasetqapy/python_questions_answers.csv']\n",
    "# df = []\n",
    "# for path in paths:\n",
    "#     data = Dataset.from_csv(path)\n",
    "#     try:\n",
    "#         df.append(data.rename_columns({'Question': 'question', 'Answer':'answer'}))\n",
    "#     except:\n",
    "#         df.append(data)\n",
    "\n",
    "# df = concatenate_datasets(df).shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:42:14.229923Z",
     "iopub.status.busy": "2024-11-16T13:42:14.229240Z",
     "iopub.status.idle": "2024-11-16T13:42:14.234204Z",
     "shell.execute_reply": "2024-11-16T13:42:14.233276Z",
     "shell.execute_reply.started": "2024-11-16T13:42:14.229876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:43:29.960691Z",
     "iopub.status.busy": "2024-11-16T13:43:29.959917Z",
     "iopub.status.idle": "2024-11-16T13:43:40.034042Z",
     "shell.execute_reply": "2024-11-16T13:43:40.033102Z",
     "shell.execute_reply.started": "2024-11-16T13:43:29.960651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "121\n",
      "['Bạn có thể sử dụng từ khóa in để kiểm tra xem một phần tử có tồn tại trong tuple hay không.']\n",
      "10.063823699951172\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# def tokenizer_function(example):\n",
    "#     prompt = ['<s>[INST]' + val['question'] + ' ?  \\n[/INST] ' + val['answer'] + \".</s>\" for val in example]\n",
    "#     x = [key for key, i in enumerate(prompt) if len(tokenizer.tokenize(i)) >= 600 and i[-1] != '?']\n",
    "#     print(len(x))\n",
    "#     y = [i for i in example['answer'] if i[-1] == '?']\n",
    "#     print(len(y))\n",
    "#     # print(x)\n",
    "#     prompt = [i for i in prompt if len(tokenizer.tokenize(i)) < 600]\n",
    "#     example = [i for key, i in enumerate(example['answer']) if key not in x]\n",
    "#     print(example[:1])\n",
    "#     inputs = tokenizer(prompt, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "#     labels = tokenizer(example, padding='max_length', truncation=True, max_length=600, return_tensors='pt')\n",
    "#     inputs['labels'] = labels['input_ids']\n",
    "#     return inputs\n",
    "\n",
    "# st  = time.time()\n",
    "# token_df = tokenizer_function(df)\n",
    "# print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:43:40.036041Z",
     "iopub.status.busy": "2024-11-16T13:43:40.035714Z",
     "iopub.status.idle": "2024-11-16T13:43:40.621266Z",
     "shell.execute_reply": "2024-11-16T13:43:40.620464Z",
     "shell.execute_reply.started": "2024-11-16T13:43:40.036008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# token_df = Dataset.from_dict(token_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:43:53.130702Z",
     "iopub.status.busy": "2024-11-16T13:43:53.130294Z",
     "iopub.status.idle": "2024-11-16T13:43:53.160051Z",
     "shell.execute_reply": "2024-11-16T13:43:53.159114Z",
     "shell.execute_reply.started": "2024-11-16T13:43:53.130652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token_df.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "# type(token_df['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:43:53.162580Z",
     "iopub.status.busy": "2024-11-16T13:43:53.161989Z",
     "iopub.status.idle": "2024-11-16T13:43:53.166681Z",
     "shell.execute_reply": "2024-11-16T13:43:53.165635Z",
     "shell.execute_reply.started": "2024-11-16T13:43:53.162535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# token_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:43:53.168060Z",
     "iopub.status.busy": "2024-11-16T13:43:53.167751Z",
     "iopub.status.idle": "2024-11-16T13:43:53.178733Z",
     "shell.execute_reply": "2024-11-16T13:43:53.177910Z",
     "shell.execute_reply.started": "2024-11-16T13:43:53.168026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train , eval_ = train_test_split(token_df, test_size=0.20, random_state=42)\n",
    "\n",
    "# train = Dataset.from_dict(train)\n",
    "# train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "# print(train)\n",
    "\n",
    "# eval_ = Dataset.from_dict(eval_)\n",
    "# eval_.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:44:26.281240Z",
     "iopub.status.busy": "2024-11-16T13:44:26.279928Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241116_134439-v7nsn9t6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/phamdinh11012004-reddit/huggingface/runs/v7nsn9t6' target=\"_blank\">./result</a></strong> to <a href='https://wandb.ai/phamdinh11012004-reddit/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/phamdinh11012004-reddit/huggingface' target=\"_blank\">https://wandb.ai/phamdinh11012004-reddit/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/phamdinh11012004-reddit/huggingface/runs/v7nsn9t6' target=\"_blank\">https://wandb.ai/phamdinh11012004-reddit/huggingface/runs/v7nsn9t6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='628' max='3480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 628/3480 37:58 < 2:53:02, 0.27 it/s, Epoch 0.54/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# agrs = TrainingArguments(\n",
    "#     output_dir = './result',\n",
    "#     per_device_train_batch_size=4,\n",
    "#     # per_device_eval_batch_size=1,\n",
    "#     # gradient_accumulation_steps=12,\n",
    "#     learning_rate=2e-4,\n",
    "#     weight_decay=3e-4,\n",
    "#     eval_strategy='steps',\n",
    "#     eval_steps=2000,\n",
    "#     logging_dir='./loss',\n",
    "#     # logging_steps=100,\n",
    "#     num_train_epochs=3,\n",
    "#     fp16=True,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=lora_model,\n",
    "#     args=agrs,\n",
    "#     train_dataset=token_df,\n",
    "#     # eval_dataset=eval_,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dcu\n",
    "# import tensorflow as tf\n",
    "\n",
    "# loss_tr = []\n",
    "# loss_eval = []\n",
    "# for event in tf.compat.v1.train.summary_iterator('/kaggle/working/loss/events.out.tfevents.1731751471.64a4e43df86f.30.0'):\n",
    "#     for value in event.summary.value:\n",
    "#         if 'train/loss' in value.tag:\n",
    "#             loss_tr.append([event.step, value.simple_value])\n",
    "#         elif 'eval/loss' in value.tag:\n",
    "#             loss_eval.append([value.simple_value])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.DataFrame(l, columns=['step','train','eval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df.to_csv(\"loss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#lưu adapter QloRA\n",
    "# lora_model.save_pretrained('adapter', save_adapter=True,save_config =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T12:17:23.889131Z",
     "iopub.status.busy": "2024-11-28T12:17:23.888500Z",
     "iopub.status.idle": "2024-11-28T12:17:33.336627Z",
     "shell.execute_reply": "2024-11-28T12:17:33.335687Z",
     "shell.execute_reply.started": "2024-11-28T12:17:23.889088Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T12:17:33.338511Z",
     "iopub.status.busy": "2024-11-28T12:17:33.338222Z",
     "iopub.status.idle": "2024-11-28T12:20:29.764778Z",
     "shell.execute_reply": "2024-11-28T12:20:29.764049Z",
     "shell.execute_reply.started": "2024-11-28T12:17:33.338458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3da667656984398a3ae0c5ed8b952c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0414368c3d414f88929a87f237c4bf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/6.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7312b9cc31c545cdb0042c7908617a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04de58178c8f4d4f9f68b0c07d375035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558a8e95e3f8403fb1ccbb1e673c33b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"model-attribution-challenge/bloom-2b5\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('model-attribution-challenge/bloom-2b5', return_tensors='pt', padding='max-length', max_length=512)\n",
    "\n",
    "# model_lora = PeftModel.from_pretrained(model, '/content/ada-qlora') #chay tren gg colab  \n",
    "model_lora = PeftModel.from_pretrained(model, '/kaggle/working/ada-qlora') #chay tren kaggle \n",
    "# model.merge_and_unload()\n",
    "\n",
    "# model.save_pretrained('merged-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T12:20:56.237454Z",
     "iopub.status.busy": "2024-11-28T12:20:56.237125Z",
     "iopub.status.idle": "2024-11-28T12:20:56.243636Z",
     "shell.execute_reply": "2024-11-28T12:20:56.242718Z",
     "shell.execute_reply.started": "2024-11-28T12:20:56.237423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def input_sent(prompt):\n",
    "    # while True:\n",
    "      # prompt = input(\"Bạn: \")\n",
    "      prompt = prompt[0].upper() + prompt[1:]\n",
    "      prompt = prompt.strip()\n",
    "      st = time.time()\n",
    "      if prompt.lower() == 'exit':\n",
    "        return\n",
    "      prompt = '<s>[INST]' + prompt + ' ?  \\n[/INST] '\n",
    "      # model_lora.to('cuda')\n",
    "      inputs = tokenizer(prompt, return_tensors='pt')\n",
    "      output = model_lora.generate(\n",
    "          **inputs,\n",
    "#           max_length=50,\n",
    "          min_length=10,\n",
    "          max_new_tokens=512,\n",
    "          # top_k = 10,\n",
    "          top_p = 0.95,\n",
    "          do_sample=True,\n",
    "          repetition_penalty=1.05,\n",
    "          \n",
    "        )\n",
    "      print(tokenizer.decode(output[0], skip_special_tokens=True).replace('\\\\n', '\\n').replace('\\\\t', '\\t')) \n",
    "      print(time.time() - st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T12:20:44.767063Z",
     "iopub.status.busy": "2024-11-28T12:20:44.766718Z",
     "iopub.status.idle": "2024-11-28T12:20:45.001648Z",
     "shell.execute_reply": "2024-11-28T12:20:45.000462Z",
     "shell.execute_reply.started": "2024-11-28T12:20:44.767034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'prompt' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m questions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Python là gì? Tại sao Python được sử dụng phổ biến?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. Làm thế nào để cài đặt Python trên máy tính?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20. Sự khác biệt giữa mutable và immutable trong Python?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, question \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(questions, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     \u001b[43minput_sent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# input_sent()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36minput_sent\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput_sent\u001b[39m(q):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# while True:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m       \u001b[38;5;66;03m# prompt = input(\"Bạn: \")\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m       prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m+\u001b[39m prompt[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      7\u001b[0m       prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      8\u001b[0m       st \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'prompt' referenced before assignment"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"1. Python là gì? Tại sao Python được sử dụng phổ biến?\",\n",
    "    \"2. Làm thế nào để cài đặt Python trên máy tính?\",\n",
    "    \"3. Biến trong Python là gì? Cách khai báo biến?\",\n",
    "    \"4. Sự khác biệt giữa Python 2 và Python 3 là gì?\",\n",
    "    \"5. Các kiểu dữ liệu cơ bản trong Python là gì?\",\n",
    "    \"6. Làm thế nào để tạo một list và thêm phần tử vào list?\",\n",
    "    \"7. Sự khác biệt giữa list và tuple trong Python?\",\n",
    "    \"8. Lambda function là gì? Khi nào nên sử dụng nó?\",\n",
    "    \"9. Sự khác biệt giữa `==` và `is` trong Python là gì?\",\n",
    "    \"10. Làm thế nào để đọc và ghi file trong Python?\",\n",
    "    \"11. Exception handling trong Python là gì? Sử dụng `try`, `except`, `finally` như thế nào?\",\n",
    "    \"12. Làm thế nào để định nghĩa một hàm (function) trong Python?\",\n",
    "    \"13. Làm thế nào để sử dụng vòng lặp `for` và `while` trong Python?\",\n",
    "    \"14. List comprehension là gì? Tại sao nên sử dụng nó?\",\n",
    "    \"15. Sự khác biệt giữa `break`, `continue` và `pass` trong vòng lặp?\",\n",
    "    \"16. Module và package trong Python là gì? Làm thế nào để sử dụng chúng?\",\n",
    "    \"17. Sự khác biệt giữa hàm `append()` và `extend()` của list?\",\n",
    "    \"18. Làm thế nào để cài đặt và sử dụng các thư viện bên ngoài bằng `pip`?\",\n",
    "    \"19. Decorator trong Python là gì? Khi nào nên sử dụng decorator?\",\n",
    "    \"20. Sự khác biệt giữa mutable và immutable trong Python?\"\n",
    "]\n",
    "\n",
    "for idx, question in enumerate(questions, start=1):\n",
    "    input_sent(question)\n",
    "\n",
    "# input_sent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #tạo api để backend call đến\n",
    "\n",
    "# from flask import Flask, request, jsonify\n",
    "# from pyngrok import ngrok\n",
    "# import time \n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# @app.route('/', methods=['POST'])\n",
    "# def predict():\n",
    "#   st = time.time()\n",
    "#   data = request.get_json()['input']\n",
    "#   data = \"Câu hỏi: \" + data + \" ?\\n\\nCâu trả lời: \"\n",
    "#   inputs = tokenizer(data, return_tensors='pt').to('cuda')\n",
    "#   model_lora.to('cuda')\n",
    "#   out = model_lora.generate(\n",
    "#       **inputs,\n",
    "#       # top_p = 0.85,\n",
    "#       # top_k = 5,\n",
    "#       # do_sample=True,\n",
    "#       # repetition_penalty = 1.05,\n",
    "#       max_new_tokens=80,\n",
    "#   )\n",
    "#   response = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
    "#   end = time.time()\n",
    "#   print(response, end - st)\n",
    "#   return jsonify({\n",
    "#       'response':response.replace('\\\\n', '\\n').replace(\"\\\\t\", \"\\t\").split('endtoken')[0]\n",
    "#   })\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   ngrok.set_auth_token(\"thay bằng token của tk ngrok\")\n",
    "#   ngrok_tunnel = ngrok.connect(5000)\n",
    "#   print('Public URL:', ngrok_tunnel.public_url)#url api đây\n",
    "#   app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5824020,
     "sourceId": 9557685,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5825104,
     "sourceId": 9559227,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6091721,
     "sourceId": 9913604,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 166216,
     "modelInstanceId": 143627,
     "sourceId": 168830,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
